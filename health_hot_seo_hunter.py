#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…»ç”Ÿ/é¥®é£Ÿçƒ­ç‚¹ + SEOé•¿å°¾è¯æŒ–æ˜å™¨ v2.0
å¤šæ•°æ®æºæŠ“å– + æ„å›¾è¯†åˆ« + è¯„åˆ†æ’åº + å†å²å¯¹æ¯”
"""

import requests
from bs4 import BeautifulSoup
import json
import re
import time
import random
from urllib.parse import quote
from datetime import datetime
import csv
import os
from pathlib import Path

# ==================== é…ç½® ====================
SEED_KEYWORDS = [
    "å…»ç”Ÿ", "é¥®é£Ÿ", "æ§ç³–", "æŠ—ç‚é¥®é£Ÿ", "å‡è„‚é¤",
    "ç¥›æ¹¿", "è¡¥æ°”è¡€", "å…»èƒƒ", "ç†¬å¤œ"
]

OUTPUT_DIR = "output"
HISTORY_DIR = "history"
CONFIG_FILE = "config.json"

# è¯·æ±‚é…ç½®ï¼ˆé¿å…è¢«å°ï¼‰
HEADERS_LIST = [
    {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"},
    {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"},
    {"User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1"}
]

TIMEOUT = 10


# ==================== å·¥å…·å‡½æ•° ====================
def get_random_headers():
    """è·å–éšæœºè¯·æ±‚å¤´"""
    headers = random.choice(HEADERS_LIST).copy()
    headers["Accept"] = "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
    headers["Accept-Language"] = "zh-CN,zh;q=0.9,en;q=0.8"
    return headers


def safe_request(url, params=None, source_name=""):
    """å®‰å…¨è¯·æ±‚ï¼Œå¤±è´¥è¿”å›None"""
    try:
        time.sleep(random.uniform(0.3, 1.0))  # å‡å°‘å»¶è¿Ÿ
        headers = get_random_headers()
        resp = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)
        if resp.status_code == 200:
            return resp
        print(f"  âš ï¸  {source_name}: HTTP {resp.status_code}")
        return None
    except Exception as e:
        print(f"  âš ï¸  {source_name}: {type(e).__name__}")
        return None


def calculate_score(keyword):
    """è®¡ç®—å…³é”®è¯æ¨èæŒ‡æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    score = 0

    # ç–‘é—®é«˜æ„å›¾ï¼ˆæƒé‡æå‡ï¼‰
    if any(w in keyword for w in ["æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "çœŸçš„", "å‰¯ä½œç”¨", "å±å®³", "èƒ½ä¸èƒ½", "å¤šä¹…", "æœ‰ç”¨å—", "æœ‰æ•ˆå—", "æ˜¯å¦", "å¦‚ä½•"]):
        score += 6

    # è´­ä¹°æ„å›¾
    if any(w in keyword for w in ["æ’è¡Œæ¦œ", "æ¨è", "å“ªä¸ªç‰Œå­", "ä»·æ ¼", "æ€ä¹ˆä¹°", "æµ‹è¯„", "äº¬ä¸œ", "æ·˜å®", "è´­ä¹°", "å“ªé‡Œä¹°"]):
        score += 5

    # é•¿å°¾è¯ï¼ˆåˆ†çº§åŠ åˆ†ï¼‰
    length = len(keyword)
    if length >= 10:
        score += 4
    elif length >= 8:
        score += 3
    elif length >= 6:
        score += 2

    # é¢†åŸŸå¼ºç›¸å…³
    if any(w in keyword for w in ["æ§ç³–", "æŠ—ç‚", "å‡è„‚", "ç¥›æ¹¿", "å…»èƒƒ", "è¡¥æ°”è¡€", "ç†¬å¤œ", "å…»ç”Ÿ", "é¥®é£Ÿ"]):
        score += 3

    # ç´§è¿«æ„Ÿå…³é”®è¯
    if any(w in keyword for w in ["å¿«é€Ÿ", "ç«‹å³", "é©¬ä¸Š", "ç´§æ€¥", "æœ€ä½³", "æœ€å¥½", "å¿…é¡»"]):
        score += 2

    # æ•°å­—å…³é”®è¯ï¼ˆé€šå¸¸æ›´å…·ä½“ï¼‰
    if re.search(r'\d+', keyword):
        score += 1

    return score


def detect_intent(keyword):
    """æ£€æµ‹æ„å›¾æ ‡ç­¾ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    intents = []

    if any(w in keyword for w in ["æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "ä»€ä¹ˆ", "æ˜¯å¦", "èƒ½ä¸èƒ½", "æœ‰ç”¨å—", "æœ‰æ•ˆå—", "æ–¹æ³•"]):
        intents.append("ç–‘é—®")
    if any(w in keyword for w in ["åŠŸæ•ˆ", "ä½œç”¨", "å¥½å¤„", "ç›Šå¤„", "æ•ˆæœ"]):
        intents.append("åŠŸæ•ˆ")
    if any(w in keyword for w in ["å‰¯ä½œç”¨", "å±å®³", "é£é™©", "ç¦å¿Œ", "æ³¨æ„äº‹é¡¹"]):
        intents.append("å‰¯ä½œç”¨")
    if any(w in keyword for w in ["æ’è¡Œæ¦œ", "æ¨è", "å“ªä¸ªç‰Œå­", "ä»·æ ¼", "æ€ä¹ˆä¹°", "æµ‹è¯„", "è´­ä¹°", "å“ªé‡Œä¹°"]):
        intents.append("è´­ä¹°")
    if any(w in keyword for w in ["å’Œ", "vs", "VS", "è¿˜æ˜¯", "å¯¹æ¯”", "åŒºåˆ«"]):
        intents.append("å¯¹æ¯”")
    if any(w in keyword for w in ["é£Ÿè°±", "èœå•", "åƒä»€ä¹ˆ", "åšæ³•"]):
        intents.append("é£Ÿè°±")

    return "/".join(intents) if intents else "é€šç”¨"


def generate_catchy_title(keyword, intent):
    """ç”Ÿæˆçˆ†æ¬¾æ ‡é¢˜å»ºè®®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    templates = {
        "ç–‘é—®": [
            f"{keyword}ï¼ŸçœŸç›¸è®©äººæ„å¤–",
            f"åŒ»ç”Ÿä¸è¯´ï¼Œä½†{keyword}ä½ å¿…é¡»çŸ¥é“",
            f"90%çš„äººéƒ½ä¸çŸ¥é“çš„{keyword}çœŸç›¸",
            f"{keyword}ï¼çœ‹å®Œè¿™ç¯‡ä½ å°±æ‡‚äº†",
        ],
        "åŠŸæ•ˆ": [
            f"{keyword}çš„5ä¸ªç¥å¥‡æ•ˆæœï¼Œç¬¬3ä¸ªå¾ˆå¤šäººä¸çŸ¥é“",
            f"åšæŒ{keyword}ï¼Œ30å¤©åèº«ä½“çš„å˜åŒ–",
            f"ä¸ºä»€ä¹ˆæ˜æ˜Ÿéƒ½åœ¨{keyword}ï¼Ÿæ•ˆæœæƒŠäºº",
        ],
        "å‰¯ä½œç”¨": [
            f"{keyword}çš„å‰¯ä½œç”¨ï¼Œå†ä¸çŸ¥é“å°±æ™šäº†",
            f"åˆ«ä¹±{keyword}ï¼è¿™3ç±»äººè¦æ³¨æ„",
            f"{keyword}çš„ç¦å¿Œï¼Œå¾ˆå¤šäººç¬¬ä¸€ä¸ªå°±é”™äº†",
        ],
        "è´­ä¹°": [
            f"{keyword}æ’è¡Œæ¦œTOP5ï¼Œç¬¬1åæ²¡æƒ³åˆ°",
            f"ä¹°å‰å¿…çœ‹ï¼{keyword}é¿å‘æŒ‡å—",
            f"{keyword}æ€ä¹ˆé€‰ï¼Ÿå†…è¡Œäººå‘Šè¯‰ä½ çœŸç›¸",
        ],
        "å¯¹æ¯”": [
            f"{keyword}ï¼šä¸€æ–‡çœ‹æ‡‚åŒºåˆ«",
            f"åˆ°åº•é€‰å“ªä¸ªï¼Ÿ{keyword}æ·±åº¦å¯¹æ¯”",
            f"åˆ«å†çº ç»“äº†ï¼{keyword}é€‰å“ªä¸ªæœ€å¥½",
        ],
        "é£Ÿè°±": [
            f"{keyword}å¤§å…¨ï¼Œ7å¤©ä¸é‡æ ·",
            f"è¥å…»å¸ˆçš„{keyword}ç§˜è¯€",
            f"7å¤©{keyword}è®¡åˆ’ï¼Œæ•ˆæœçœ‹å¾—è§",
        ],
        "é€šç”¨": [
            f"{keyword}ï¼šæ–°æ‰‹å®Œå…¨æŒ‡å—",
            f"å…³äº{keyword}ï¼Œä½ éœ€è¦çŸ¥é“çš„ä¸€åˆ‡",
            f"{keyword}çš„æ­£ç¡®æ‰“å¼€æ–¹å¼",
        ]
    }

    intent_key = intent.split("/")[0] if intent else "é€šç”¨"
    templates_list = templates.get(intent_key, templates["é€šç”¨"])
    return random.choice(templates_list)


def save_history(keywords, topics):
    """ä¿å­˜å†å²æ•°æ®"""
    os.makedirs(HISTORY_DIR, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    history_data = {
        "timestamp": timestamp,
        "datetime": datetime.now().isoformat(),
        "keywords_count": len(keywords),
        "topics_count": len(topics),
        "top_keywords": [kw for kw, _ in list(keywords.items())[:20]],
        "topics": [t["title"] for t in topics[:10]]
    }

    history_file = f"{HISTORY_DIR}/history_{timestamp}.json"
    with open(history_file, "w", encoding="utf-8") as f:
        json.dump(history_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… å·²ä¿å­˜å†å²æ•°æ®: {history_file}")


def load_recent_history(limit=5):
    """åŠ è½½æœ€è¿‘çš„å†å²è®°å½•"""
    if not os.path.exists(HISTORY_DIR):
        return []

    files = sorted(Path(HISTORY_DIR).glob("history_*.json"), reverse=True)[:limit]
    history = []

    for file in files:
        try:
            with open(file, "r", encoding="utf-8") as f:
                data = json.load(f)
                history.append(data)
        except:
            pass

    return history


# ==================== æ•°æ®æºæŠ“å– ====================
def fetch_baidu_suggestions(keyword):
    """ç™¾åº¦ä¸‹æ‹‰è¯"""
    url = "http://suggestion.baidu.com/su"
    params = {"wd": keyword, "cb": "cb"}

    print(f"  ğŸ” ç™¾åº¦: {keyword}")
    resp = safe_request(url, params, "ç™¾åº¦ä¸‹æ‹‰")

    if not resp:
        return []

    try:
        text = resp.text
        match = re.search(r'cb\((.*)\)', text)
        if match:
            json_str = match.group(1)
            try:
                json_str = json_str.encode('latin1').decode('gb2312')
            except:
                pass
            data = json.loads(json_str)
            if isinstance(data, dict) and "s" in data:
                return data.get("s", [])
    except:
        pass
    return []


def fetch_bilibili_suggestions(keyword):
    """Bç«™æœç´¢å»ºè®®"""
    url = "https://s.search.bilibili.com/main/suggest"
    params = {"term": keyword}

    print(f"  ğŸ” Bç«™: {keyword}")
    resp = safe_request(url, params, "Bç«™å»ºè®®")

    if not resp:
        return []

    try:
        data = resp.json()
        if isinstance(data, dict) and "result" in data:
            return [item.get("value", "") for item in data.get("result", []) if "value" in item]
    except:
        pass
    return []


def fetch_taobao_suggestions(keyword):
    """æ·˜å®æœç´¢å»ºè®®"""
    url = "https://suggest.taobao.com/sug"
    params = {"q": keyword, "code": "utf-8"}

    print(f"  ğŸ” æ·˜å®: {keyword}")
    resp = safe_request(url, params, "æ·˜å®å»ºè®®")

    if not resp:
        return []

    try:
        data = resp.json()
        return data.get("result", [])
    except:
        pass
    return []


def fetch_zhihu_hot():
    """çŸ¥ä¹çƒ­æ¦œ"""
    url = "https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total"

    print(f"  ğŸ” çŸ¥ä¹çƒ­æ¦œ")
    resp = safe_request(url, source_name="çŸ¥ä¹çƒ­æ¦œ")

    if not resp:
        return []

    try:
        data = resp.json()
        items = data.get("data", [])
        results = []
        for item in items[:30]:
            target = item.get("target", {})
            title = target.get("title", "")
            if title:
                results.append(title)
        return results
    except:
        pass
    return []


def fetch_weibo_hot():
    """å¾®åšçƒ­æœ"""
    url = "https://weibo.com/ajax/side/hotSearch"

    print(f"  ğŸ” å¾®åšçƒ­æœ")
    resp = safe_request(url, source_name="å¾®åšçƒ­æœ")

    if not resp:
        return []

    try:
        data = resp.json()
        items = data.get("data", {}).get("realtime", [])
        return [item.get("word", "") for item in items]
    except:
        pass
    return []


# ==================== å¤‡ç”¨æ•°æ®ç”Ÿæˆ ====================
def generate_fallback_keywords():
    """ç”Ÿæˆå¤‡ç”¨å…³é”®è¯"""
    prefixes = ["æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "æœ€å¥½çš„", "å¿«é€Ÿ", "æœ‰æ•ˆ", "å®‰å…¨", "ç§‘å­¦"]
    suffixes = ["æ–¹æ³•", "é£Ÿè°±", "é£Ÿç‰©", "æ°´æœ", "è”¬èœ", "èŒ¶", "æ³¨æ„äº‹é¡¹", "å±å®³", "å¥½å¤„", "æ—¶é—´", "æ’è¡Œæ¦œ", "æ¨è", "ç¦å¿Œ", "é£Ÿè°±å¤§å…¨", "ä¸€å‘¨è®¡åˆ’"]
    questions = ["çœŸçš„æœ‰ç”¨å—", "å‰¯ä½œç”¨æ˜¯ä»€ä¹ˆ", "å¤šä¹…è§æ•ˆ", "èƒ½ä¸èƒ½å¤©å¤©åƒ", "å“ªäº›äººä¸èƒ½åƒ", "ä»€ä¹ˆæ—¶å€™åƒæœ€å¥½", "æ­£ç¡®æ‰“å¼€æ–¹å¼", "é¿å‘æŒ‡å—"]

    generated = []
    for seed in SEED_KEYWORDS:
        for prefix in prefixes[:5]:
            generated.append(f"{prefix}{seed}")
        for suffix in suffixes[:8]:
            generated.append(f"{seed}{suffix}")
        for q in questions[:5]:
            generated.append(f"{seed}{q}")

    return generated


def generate_fallback_hot_topics():
    """ç”Ÿæˆå¤‡ç”¨çƒ­ç‚¹é€‰é¢˜"""
    topics = [
        "æ§ç³–é¥®é£ŸçœŸçš„èƒ½æŠ—è¡°è€å—ï¼Ÿå“ˆä½›ç ”ç©¶æ­ç¤ºçœŸç›¸",
        "æŠ—ç‚é¥®é£Ÿé£Ÿç‰©æ’è¡Œæ¦œTOP10ï¼Œç¬¬ä¸€åä½ è‚¯å®šæƒ³ä¸åˆ°",
        "ç†¬å¤œåæ€ä¹ˆè¡¥æ•‘ï¼ŸåŒ»ç”Ÿæ¨èçš„3ä¸ªé»„é‡‘æ—¶é—´ç‚¹",
        "ç¥›æ¹¿é£Ÿç‰©æ’è¡Œæ¦œï¼šçº¢è±†è–ç±³æ°´æ’ç¬¬å‡ ï¼Ÿ",
        "å‡è„‚é¤ä¸€å‘¨é£Ÿè°±ï¼Œä¸æ‰ç§¤æ˜¯å› ä¸ºä½ æ²¡åƒå¯¹",
        "å…»èƒƒé£Ÿç‰©æ’è¡Œæ¦œï¼šè¿™äº›é£Ÿç‰©è¶Šåƒèƒƒè¶Šéš¾å—",
        "è¡¥æ°”è¡€é£Ÿç‰©TOP10ï¼Œé˜¿èƒ¶çº¢æ£æ’ç¬¬å‡ ï¼Ÿ",
        "æŠ—ç‚é¥®é£Ÿvsç”Ÿé…®é¥®é£Ÿï¼Œå“ªä¸ªæ›´é€‚åˆä¸­å›½äººï¼Ÿ",
        "æ§ç³–é¥®é£Ÿä¸€å‘¨é£Ÿè°±ï¼Œå‘Šåˆ«ç³–å°¿ç—…é£é™©",
        "ç¥›æ¹¿çš„æœ€å¥½æ–¹æ³•ï¼Œä¸æ˜¯çº¢è±†è–ç±³æ°´ï¼"
    ]
    return [{"title": t, "source": "çƒ­ç‚¹æ¨¡æ‹Ÿ", "angle": "äº‰è®®/ç§‘æ™®å‹"} for t in topics]


# ==================== ä¸»é€»è¾‘ ====================
def main():
    start_time = time.time()
    print("=" * 70)
    print("ğŸµ å…»ç”Ÿ/é¥®é£Ÿçƒ­ç‚¹ + SEOé•¿å°¾è¯æŒ–æ˜å™¨ v2.0")
    print("=" * 70)

    all_keywords = {}
    hot_topics = []

    # 1. åŸºäºç§å­è¯æŠ“å–å„å¹³å°çš„å»ºè®®è¯
    print("\nğŸ“Š ç¬¬ä¸€é˜¶æ®µï¼šåŸºäºç§å­è¯æŠ“å–æœç´¢å»ºè®®")
    print("-" * 70)

    for keyword in SEED_KEYWORDS:
        print(f"\nğŸŒ± ç§å­è¯: [{keyword}]")

        # å¤šæºå¹¶è¡Œ
        baidu_results = fetch_baidu_suggestions(keyword)
        for kw in baidu_results:
            if kw not in all_keywords:
                all_keywords[kw] = {"sources": [], "score": 0, "intent": ""}
            all_keywords[kw]["sources"].append("ç™¾åº¦")

        bili_results = fetch_bilibili_suggestions(keyword)
        for kw in bili_results:
            if kw not in all_keywords:
                all_keywords[kw] = {"sources": [], "score": 0, "intent": ""}
            all_keywords[kw]["sources"].append("Bç«™")

        taobao_results = fetch_taobao_suggestions(keyword)
        for kw in taobao_results:
            if isinstance(kw, str) and kw:
                if kw not in all_keywords:
                    all_keywords[kw] = {"sources": [], "score": 0, "intent": ""}
                all_keywords[kw]["sources"].append("æ·˜å®")

        time.sleep(random.uniform(0.5, 1.0))

    # 2. æŠ“å–çƒ­æ¦œ
    print("\n\nğŸ“Š ç¬¬äºŒé˜¶æ®µï¼šæŠ“å–å¹³å°çƒ­æ¦œ")
    print("-" * 70)

    zhihu_hot = fetch_zhihu_hot()
    for title in zhihu_hot:
        if any(kw in title for kw in SEED_KEYWORDS):
            hot_topics.append({"title": title, "source": "çŸ¥ä¹çƒ­æ¦œ", "angle": "äº‰è®®/é—®é¢˜å‹"})

    weibo_hot = fetch_weibo_hot()
    if not weibo_hot:
        print("  âš ï¸  å¾®åšçƒ­æœå·²è·³è¿‡ï¼ˆåçˆ¬é™åˆ¶ï¼‰")
    else:
        for title in weibo_hot:
            if any(kw in title for kw in SEED_KEYWORDS):
                hot_topics.append({"title": title, "source": "å¾®åšçƒ­æœ", "angle": "çƒ­ç‚¹è¿½è¸ª"})

    # 3. å¦‚æœå¤–éƒ¨æºå…¨éƒ¨å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®
    if not all_keywords and not hot_topics:
        print("\n  âš ï¸  æ‰€æœ‰å¤–éƒ¨æ•°æ®æºå‡ä¸å¯ç”¨ï¼Œå¯ç”¨å¤‡ç”¨æ•°æ®ç”Ÿæˆæ–¹æ¡ˆ")
        print("-" * 70)

        fallback_keywords = generate_fallback_keywords()
        for kw in fallback_keywords:
            all_keywords[kw] = {"sources": ["å¤‡ç”¨ç”Ÿæˆ"], "score": 0, "intent": ""}
        hot_topics = generate_fallback_hot_topics()

    # 4. è®¡ç®—åˆ†æ•°å’Œæ„å›¾
    print("\n\nğŸ“Š ç¬¬ä¸‰é˜¶æ®µï¼šåˆ†æå…³é”®è¯")
    print("-" * 70)

    for kw, data in all_keywords.items():
        data["score"] = calculate_score(kw)
        data["intent"] = detect_intent(kw)
        data["catchy_title"] = generate_catchy_title(kw, data["intent"])
        data["source"] = "+".join(set(data["sources"]))

    sorted_keywords = sorted(all_keywords.items(), key=lambda x: x[1]["score"], reverse=True)
    sorted_topics = sorted(hot_topics, key=lambda x: len(x["title"]), reverse=True)

    # 5. ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
    print("\n\nğŸ“Š ç¬¬å››é˜¶æ®µï¼šç”Ÿæˆè¾“å‡ºæ–‡ä»¶")
    print("-" * 70)

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # 5.1 çƒ­ç‚¹é€‰é¢˜ TOP10
    hot_topics_md = f"""# å…»ç”Ÿ/é¥®é£Ÿçƒ­ç‚¹é€‰é¢˜ TOP10

> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
> æ•°æ®æº: {'+'.join(set([t['source'] for t in hot_topics])) if hot_topics else 'å¤‡ç”¨ç”Ÿæˆ'}

---

"""
    for i, topic in enumerate(sorted_topics[:10], 1):
        hot_topics_md += f"""## {i}. {topic['title']}

- **æ¥æº**: {topic['source']}
- **äº‰è®®ç‚¹/è§’åº¦**: {topic['angle']}
- **é€‚åˆå†™çš„è§’åº¦**: æ·±åº¦è§£æ / é¿å‘æŒ‡å— / ç§‘æ™®å‘
- **ğŸ’¥ çˆ†æ¬¾æ ‡é¢˜å»ºè®®**: {topic['title'][:40]}...è¿™ç¯‡å‘Šè¯‰ä½ çœŸç›¸

---

"""

    with open(f"{OUTPUT_DIR}/hot_topics.md", "w", encoding="utf-8") as f:
        f.write(hot_topics_md)
    print(f"âœ… å·²ç”Ÿæˆ: {OUTPUT_DIR}/hot_topics.md")

    # 5.2 SEOå…³é”®è¯ Markdownè¡¨æ ¼
    seo_keywords_md = f"""# SEOé•¿å°¾è¯æŒ–æ˜ç»“æœ

> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
> æ€»å…³é”®è¯æ•°: {len(sorted_keywords)}
> é«˜åˆ†å…³é”®è¯(â‰¥8åˆ†): {sum(1 for _, d in sorted_keywords if d['score'] >= 8)}

---

| å…³é”®è¯ | æ¥æº | æ„å›¾æ ‡ç­¾ | æ¨èæŒ‡æ•° | çˆ†æ¬¾æ ‡é¢˜å»ºè®® |
|--------|------|----------|----------|--------------|
"""
    for kw, data in sorted_keywords[:100]:
        seo_keywords_md += f"| {kw} | {data['source']} | {data['intent']} | {data['score']} | {data['catchy_title']} |\n"

    with open(f"{OUTPUT_DIR}/seo_keywords.md", "w", encoding="utf-8") as f:
        f.write(seo_keywords_md)
    print(f"âœ… å·²ç”Ÿæˆ: {OUTPUT_DIR}/seo_keywords.md")

    # 5.3 SEOå…³é”®è¯ CSV
    with open(f"{OUTPUT_DIR}/seo_keywords.csv", "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["å…³é”®è¯", "æ¥æº", "æ„å›¾æ ‡ç­¾", "æ¨èæŒ‡æ•°", "çˆ†æ¬¾æ ‡é¢˜å»ºè®®"])
        for kw, data in sorted_keywords[:100]:
            writer.writerow([kw, data["source"], data["intent"], data["score"], data["catchy_title"]])
    print(f"âœ… å·²ç”Ÿæˆ: {OUTPUT_DIR}/seo_keywords.csv")

    # 5.4 JSONæ ¼å¼ï¼ˆæ–°å¢ï¼‰
    json_output = {
        "generated_at": datetime.now().isoformat(),
        "total_keywords": len(sorted_keywords),
        "high_score_keywords": [
            {"keyword": kw, **data}
            for kw, data in sorted_keywords[:50]
        ],
        "hot_topics": sorted_topics[:10]
    }

    with open(f"{OUTPUT_DIR}/seo_keywords.json", "w", encoding="utf-8") as f:
        json.dump(json_output, f, ensure_ascii=False, indent=2)
    print(f"âœ… å·²ç”Ÿæˆ: {OUTPUT_DIR}/seo_keywords.json")

    # 6. ä¿å­˜å†å²æ•°æ®
    save_history(all_keywords, hot_topics)

    # 7. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    try:
        from charts import KeywordVisualizer

        stats = {
            "total": len(sorted_keywords),
            "high_score": sum(1 for _, d in sorted_keywords if d['score'] >= 8),
            "avg_score": sum(d['score'] for _, d in sorted_keywords) / len(sorted_keywords)
        }

        visualizer = KeywordVisualizer(output_dir=f"{OUTPUT_DIR}/charts")
        charts = visualizer.generate_all_charts(sorted_keywords, stats)

    except ImportError as e:
        print(f"\nâš ï¸  å¯è§†åŒ–æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        print("ğŸ’¡ å¦‚éœ€ç”Ÿæˆå›¾è¡¨ï¼Œè¯·è¿è¡Œ: pip install matplotlib seaborn wordcloud")
    except Exception as e:
        print(f"\nâš ï¸  å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")

    # 8. æ‰“å°é¢„è§ˆ
    elapsed = time.time() - start_time
    print("\n\n" + "=" * 70)
    print("ğŸ‰ æ•°æ®æŒ–æ˜å®Œæˆï¼")
    print("=" * 70)
    print(f"â±ï¸  è€—æ—¶: {elapsed:.1f}ç§’")
    print(f"ğŸ“Š å…³é”®è¯æ€»æ•°: {len(sorted_keywords)}")
    print(f"ğŸ”¥ é«˜åˆ†å…³é”®è¯(â‰¥8åˆ†): {sum(1 for _, d in sorted_keywords if d['score'] >= 8)}")

    print("\nğŸ“ˆ çƒ­ç‚¹é€‰é¢˜ TOP10:")
    print("-" * 70)
    for i, topic in enumerate(sorted_topics[:10], 1):
        print(f"{i:2d}. {topic['title'][:60]}")

    print("\n\nğŸ“ˆ SEOå…³é”®è¯ TOP30:")
    print("-" * 70)
    print(f"{'æ’å':<4} {'å…³é”®è¯':<28} {'åˆ†æ•°':<4} {'æ„å›¾':<12}")
    print("-" * 70)
    for i, (kw, data) in enumerate(sorted_keywords[:30], 1):
        print(f"{i:<4} {kw[:26]:<28} {data['score']:<4} {data['intent']:<12}")

    print(f"\n\nğŸ’¾ è¾“å‡ºæ–‡ä»¶ä½äº {OUTPUT_DIR}/ ç›®å½•")
    print("-" * 70)


if __name__ == "__main__":
    main()
