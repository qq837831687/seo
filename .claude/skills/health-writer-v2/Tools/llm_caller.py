#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM Caller - ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£
æ”¯æŒå¤šç§ LLM APIï¼šOpenAI, Claude, Ollama, å›½å†…APIç­‰

Version: 1.0
Created: 2026-01-20
"""

import json
import os
from typing import Dict, List, Optional, Union
from dataclasses import dataclass
import requests


@dataclass
class LLMConfig:
    """LLM é…ç½®"""
    provider: str  # openai, claude, ollama, qianwen, etc.
    api_key: str
    base_url: Optional[str] = None
    model: str = "gpt-3.5-turbo"
    temperature: float = 0.7
    max_tokens: int = 2000
    timeout: int = 60


class LLMCaller:
    """
    ç»Ÿä¸€çš„ LLM è°ƒç”¨ç±»
    æ”¯æŒå¤šç§ LLM Provider
    """

    def __init__(self, config: Optional[LLMConfig] = None):
        """
        åˆå§‹åŒ– LLM Caller

        Args:
            config: LLM é…ç½®ï¼Œå¦‚æœä¸º None åˆ™ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶åŠ è½½
        """
        if config is None:
            config = self._load_config_from_env()

        self.config = config
        self.provider = config.provider.lower()

    def _load_config_from_env(self) -> LLMConfig:
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        provider = os.getenv("LLM_PROVIDER", "openai")
        api_key = os.getenv("LLM_API_KEY", "")

        if not api_key:
            raise ValueError(
                "LLM_API_KEY not found in environment variables. "
                "Please set LLM_API_KEY or provide config."
            )

        return LLMConfig(
            provider=provider,
            api_key=api_key,
            base_url=os.getenv("LLM_BASE_URL"),
            model=os.getenv("LLM_MODEL", self._get_default_model(provider)),
            temperature=float(os.getenv("LLM_TEMPERATURE", "0.7")),
            max_tokens=int(os.getenv("LLM_MAX_TOKENS", "2000")),
            timeout=int(os.getenv("LLM_TIMEOUT", "60")),
        )

    def _get_default_model(self, provider: str) -> str:
        """è·å–é»˜è®¤æ¨¡å‹"""
        defaults = {
            "openai": "gpt-3.5-turbo",
            "claude": "claude-3-sonnet-20240229",
            "ollama": "llama2",
            "qianwen": "qwen-turbo",
        }
        return defaults.get(provider, "gpt-3.5-turbo")

    def call(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        response_format: Optional[str] = "json",  # json, text
        **kwargs
    ) -> Union[str, Dict]:
        """
        è°ƒç”¨ LLM

        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            response_format: å“åº”æ ¼å¼ï¼ˆjson æˆ– textï¼‰
            **kwargs: é¢å¤–å‚æ•°ï¼ˆè¦†ç›– configï¼‰

        Returns:
            LLM çš„å“åº”ï¼ˆæ ¹æ® response_format è¿”å›å­—ç¬¦ä¸²æˆ–å­—å…¸ï¼‰
        """
        # åˆå¹¶é…ç½®
        config = self.config
        for key, value in kwargs.items():
            if hasattr(config, key):
                setattr(config, key, value)

        # æ ¹æ®ä¸åŒçš„ provider è°ƒç”¨ä¸åŒçš„å®ç°
        if self.provider == "openai":
            return self._call_openai(prompt, system_prompt, response_format, config)
        elif self.provider == "claude":
            return self._call_claude(prompt, system_prompt, response_format, config)
        elif self.provider == "ollama":
            return self._call_ollama(prompt, system_prompt, response_format, config)
        elif self.provider in ["qianwen", "dashscope", "alibaba"]:
            return self._call_qianwen(prompt, system_prompt, response_format, config)
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")

    def _call_openai(
        self,
        prompt: str,
        system_prompt: Optional[str],
        response_format: str,
        config: LLMConfig
    ) -> Union[str, Dict]:
        """è°ƒç”¨ OpenAI API"""
        import openai

        client = openai.OpenAI(
            api_key=config.api_key,
            base_url=config.base_url,
            timeout=config.timeout,
        )

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        # OpenAI éœ€è¦è®¾ç½® response_format
        kwargs = {
            "model": config.model,
            "messages": messages,
            "temperature": config.temperature,
            "max_tokens": config.max_tokens,
        }

        # å¦‚æœéœ€è¦ JSON è¾“å‡º
        if response_format == "json":
            kwargs["response_format"] = {"type": "json_object"}

        response = client.chat.completions.create(**kwargs)

        content = response.choices[0].message.content

        if response_format == "json":
            try:
                return json.loads(content)
            except json.JSONDecodeError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
                return content
        else:
            return content

    def _call_claude(
        self,
        prompt: str,
        system_prompt: Optional[str],
        response_format: str,
        config: LLMConfig
    ) -> Union[str, Dict]:
        """è°ƒç”¨ Claude API"""
        import anthropic

        client = anthropic.Anthropic(
            api_key=config.api_key,
            timeout=config.timeout,
        )

        messages = [{"role": "user", "content": prompt}]

        # å¦‚æœéœ€è¦ JSON è¾“å‡ºï¼Œåœ¨æç¤ºè¯ä¸­è¯´æ˜
        if response_format == "json":
            if system_prompt:
                system_prompt += "\n\nè¯·åŠ¡å¿…ä»¥ JSON æ ¼å¼è¾“å‡ºã€‚"
            else:
                prompt += "\n\nè¯·åŠ¡å¿…ä»¥ JSON æ ¼å¼è¾“å‡ºã€‚"

        response = client.messages.create(
            model=config.model,
            max_tokens=config.max_tokens,
            temperature=config.temperature,
            system=system_prompt,
            messages=messages,
        )

        content = response.content[0].text

        if response_format == "json":
            try:
                return json.loads(content)
            except json.JSONDecodeError:
                return content
        else:
            return content

    def _call_ollama(
        self,
        prompt: str,
        system_prompt: Optional[str],
        response_format: str,
        config: LLMConfig
    ) -> Union[str, Dict]:
        """è°ƒç”¨ Ollama (æœ¬åœ°) API"""
        base_url = config.base_url or "http://localhost:11434"
        url = f"{base_url}/api/generate"

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        payload = {
            "model": config.model,
            "prompt": prompt,
            "system": system_prompt,
            "stream": False,
            "options": {
                "temperature": config.temperature,
                "num_predict": config.max_tokens,
            }
        }

        response = requests.post(url, json=payload, timeout=config.timeout)
        response.raise_for_status()

        content = response.json().get("response", "")

        if response_format == "json":
            try:
                return json.loads(content)
            except json.JSONDecodeError:
                return content
        else:
            return content

    def _call_qianwen(
        self,
        prompt: str,
        system_prompt: Optional[str],
        response_format: str,
        config: LLMConfig
    ) -> Union[str, Dict]:
        """è°ƒç”¨é€šä¹‰åƒé—® API"""
        base_url = config.base_url or "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"

        headers = {
            "Authorization": f"Bearer {config.api_key}",
            "Content-Type": "application/json",
        }

        # æ„å»ºæ¶ˆæ¯
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        payload = {
            "model": config.model or "qwen-turbo",
            "input": {
                "messages": messages
            },
            "parameters": {
                "temperature": config.temperature,
                "max_tokens": config.max_tokens,
                "result_format": response_format.upper(),
            }
        }

        response = requests.post(base_url, json=payload, headers=headers, timeout=config.timeout)
        response.raise_for_status()

        result = response.json()

        # é€šä¹‰åƒé—®çš„å“åº”æ ¼å¼
        if result.get("output"):
            content = result["output"].get("text", "")
            if response_format == "json":
                try:
                    return json.loads(content)
                except json.JSONDecodeError:
                    return content
            else:
                return content
        else:
            raise ValueError(f"Unexpected response format: {result}")


# ============================================================================
# ä¾¿æ·å‡½æ•°
# ============================================================================

def create_llm_caller(
    provider: str = "openai",
    api_key: Optional[str] = None,
    **kwargs
) -> LLMCaller:
    """
    åˆ›å»º LLM Caller çš„ä¾¿æ·å‡½æ•°

    Args:
        provider: LLM Provider (openai, claude, ollama, qianwen)
        api_key: API Keyï¼ˆå¦‚æœä¸º Noneï¼Œä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        **kwargs: å…¶ä»–é…ç½®å‚æ•°

    Returns:
        LLMCaller å®ä¾‹
    """
    if api_key is None:
        api_key = os.getenv("LLM_API_KEY", "")

    if not api_key:
        raise ValueError("api_key must be provided or set LLM_API_KEY environment variable")

    config = LLMConfig(
        provider=provider,
        api_key=api_key,
        **kwargs
    )

    return LLMCaller(config)


# ============================================================================
# ä¸»ç¨‹åºï¼ˆç”¨äºæµ‹è¯•ï¼‰
# ============================================================================

def main():
    """æµ‹è¯• LLM Caller"""

    print("=" * 60)
    print("LLM Caller - æµ‹è¯•ç¨‹åº")
    print("=" * 60)

    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    api_key = os.getenv("LLM_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯: è¯·è®¾ç½® LLM_API_KEY ç¯å¢ƒå˜é‡")
        print("\nç¤ºä¾‹:")
        print("export LLM_API_KEY='your-api-key'")
        print("export LLM_PROVIDER='openai'  # æˆ– claude, ollama, qianwen")
        return

    provider = os.getenv("LLM_PROVIDER", "openai")
    print(f"âœ… ä½¿ç”¨ Provider: {provider}")

    # åˆ›å»º caller
    try:
        caller = create_llm_caller(provider=provider)
        print(f"âœ… LLM Caller åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆ›å»ºå¤±è´¥: {e}")
        return

    # æµ‹è¯•è°ƒç”¨
    print("\nğŸ“ æµ‹è¯•ç®€å•è°ƒç”¨...")
    try:
        response = caller.call(
            prompt="ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚",
            response_format="text"
        )
        print(f"âœ… è°ƒç”¨æˆåŠŸï¼")
        print(f"å“åº”: {response}")
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤±è´¥: {e}")

    # æµ‹è¯• JSON è°ƒç”¨
    print("\nğŸ“ æµ‹è¯• JSON æ ¼å¼è°ƒç”¨...")
    try:
        response = caller.call(
            prompt="è¯·ç”¨ JSON æ ¼å¼è¿”å›ï¼š{'name': 'æµ‹è¯•', 'age': 25}",
            response_format="json"
        )
        print(f"âœ… è°ƒç”¨æˆåŠŸï¼")
        print(f"å“åº”: {json.dumps(response, ensure_ascii=False, indent=2)}")
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤±è´¥: {e}")


if __name__ == "__main__":
    main()
