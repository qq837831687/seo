#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆç½‘ç»œæŠ“å–æ¨¡å—
æ”¯æŒå¤šç§åçˆ¬ç­–ç•¥ï¼šä»£ç†æ± ã€çœŸå®æµè§ˆå™¨è¯·æ±‚å¤´ã€Cookieç®¡ç†ã€æ™ºèƒ½é‡è¯•
"""

import requests
from bs4 import BeautifulSoup
import json
import re
import time
import random
from typing import List, Dict, Optional
from urllib.parse import quote, urlencode
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


class EnhancedFetcher:
    """å¢å¼ºç‰ˆç½‘ç»œæŠ“å–å™¨"""

    # çœŸå®æµè§ˆå™¨è¯·æ±‚å¤´æ± 
    HEADERS_POOL = [
        {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Cache-Control": "max-age=0"
        },
        {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive"
        },
        {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive"
        }
    ]

    def __init__(self, use_proxy=False, proxy_list=None):
        """
        åˆå§‹åŒ–æŠ“å–å™¨

        Args:
            use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
            proxy_list: ä»£ç†åˆ—è¡¨ [{'http': '...', 'https': '...'}]
        """
        self.use_proxy = use_proxy
        self.proxy_list = proxy_list or []
        self.session = requests.Session()
        self.session.cookies = requests.cookies.RequestsCookieJar()

        # é…ç½®é‡è¯•ç­–ç•¥
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry

        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

    def _get_random_headers(self):
        """è·å–éšæœºè¯·æ±‚å¤´"""
        headers = random.choice(self.HEADERS_POOL).copy()
        return headers

    def _get_proxy(self):
        """è·å–éšæœºä»£ç†"""
        if not self.use_proxy or not self.proxy_list:
            return None
        return random.choice(self.proxy_list)

    def fetch(self, url: str, params: dict = None, method: str = "GET",
              timeout: int = 15, source_name: str = "") -> Optional[requests.Response]:
        """
        å®‰å…¨çš„HTTPè¯·æ±‚

        Args:
            url: è¯·æ±‚URL
            params: æŸ¥è¯¢å‚æ•°
            method: è¯·æ±‚æ–¹æ³• GET/POST
            timeout: è¶…æ—¶æ—¶é—´
            source_name: æ•°æ®æºåç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            Responseå¯¹è±¡æˆ–None
        """
        headers = self._get_random_headers()
        proxies = self._get_proxy()

        # éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        delay = random.uniform(1.0, 3.0)
        time.sleep(delay)

        try:
            if method.upper() == "GET":
                response = self.session.get(
                    url,
                    params=params,
                    headers=headers,
                    proxies=proxies,
                    timeout=timeout
                )
            else:
                response = self.session.post(
                    url,
                    data=params,
                    headers=headers,
                    proxies=proxies,
                    timeout=timeout
                )

            if response.status_code == 200:
                logger.info(f"âœ… {source_name}: æˆåŠŸ")
                return response
            else:
                logger.warning(f"âš ï¸  {source_name}: HTTP {response.status_code}")
                return None

        except requests.exceptions.Timeout:
            logger.warning(f"âš ï¸  {source_name}: è¶…æ—¶")
            return None
        except requests.exceptions.RequestException as e:
            logger.warning(f"âš ï¸  {source_name}: {type(e).__name__}")
            return None
        except Exception as e:
            logger.warning(f"âš ï¸  {source_name}: æœªçŸ¥é”™è¯¯ - {str(e)[:50]}")
            return None


class BaiduFetcher(EnhancedFetcher):
    """ç™¾åº¦æœç´¢å»ºè®®æŠ“å–å™¨"""

    def fetch_suggestions(self, keyword: str) -> List[str]:
        """
        æŠ“å–ç™¾åº¦æœç´¢å»ºè®®

        Args:
            keyword: å…³é”®è¯

        Returns:
            å»ºè®®è¯åˆ—è¡¨
        """
        url = "http://suggestion.baidu.com/su"
        params = {"wd": keyword, "cb": "cb"}

        logger.info(f"  ğŸ” ç™¾åº¦: {keyword}")
        response = self.fetch(url, params=params, source_name="ç™¾åº¦ä¸‹æ‹‰")

        if not response:
            return []

        try:
            # ç™¾åº¦è¿”å›çš„æ˜¯GB2312ç¼–ç çš„JSONP
            text = response.text
            match = re.search(r'cb\((.*)\)', text)
            if match:
                json_str = match.group(1)
                # å°è¯•è§£ç GB2312
                try:
                    json_str = json_str.encode('latin1').decode('gb2312')
                except:
                    pass

                data = json.loads(json_str)
                if isinstance(data, dict) and "s" in data:
                    suggestions = data.get("s", [])
                    logger.info(f"  âœ… ç™¾åº¦: è·å– {len(suggestions)} ä¸ªå»ºè®®è¯")
                    return suggestions
        except Exception as e:
            logger.warning(f"  âš ï¸  ç™¾åº¦è§£æå¤±è´¥: {e}")

        return []


class BilibiliFetcher(EnhancedFetcher):
    """Bç«™æœç´¢å»ºè®®æŠ“å–å™¨"""

    def fetch_suggestions(self, keyword: str) -> List[str]:
        """æŠ“å–Bç«™æœç´¢å»ºè®®"""
        url = "https://s.search.bilibili.com/main/suggest"
        params = {"term": keyword}

        logger.info(f"  ğŸ” Bç«™: {keyword}")
        response = self.fetch(url, params=params, source_name="Bç«™å»ºè®®")

        if not response:
            return []

        try:
            data = response.json()
            if isinstance(data, dict) and "result" in data:
                suggestions = [item.get("value", "") for item in data.get("result", []) if "value" in item]
                logger.info(f"  âœ… Bç«™: è·å– {len(suggestions)} ä¸ªå»ºè®®è¯")
                return suggestions
        except Exception as e:
            logger.warning(f"  âš ï¸  Bç«™è§£æå¤±è´¥: {e}")

        return []


class TaobaoFetcher(EnhancedFetcher):
    """æ·˜å®æœç´¢å»ºè®®æŠ“å–å™¨"""

    def fetch_suggestions(self, keyword: str) -> List[str]:
        """æŠ“å–æ·˜å®æœç´¢å»ºè®®"""
        url = "https://suggest.taobao.com/sug"
        params = {"q": keyword, "code": "utf-8"}

        logger.info(f"  ğŸ” æ·˜å®: {keyword}")
        response = self.fetch(url, params=params, source_name="æ·˜å®å»ºè®®")

        if not response:
            return []

        try:
            data = response.json()
            result = data.get("result", [])
            suggestions = []
            for item in result:
                if isinstance(item, list) and len(item) > 0:
                    suggestions.append(item[0])
            logger.info(f"  âœ… æ·˜å®: è·å– {len(suggestions)} ä¸ªå»ºè®®è¯")
            return suggestions
        except Exception as e:
            logger.warning(f"  âš ï¸  æ·˜å®è§£æå¤±è´¥: {e}")

        return []


class ZhihuFetcher(EnhancedFetcher):
    """çŸ¥ä¹çƒ­æ¦œæŠ“å–å™¨"""

    def fetch_hot_topics(self) -> List[str]:
        """æŠ“å–çŸ¥ä¹çƒ­æ¦œ"""
        url = "https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total"

        logger.info(f"  ğŸ” çŸ¥ä¹çƒ­æ¦œ")
        response = self.fetch(url, source_name="çŸ¥ä¹çƒ­æ¦œ")

        if not response:
            return []

        try:
            data = response.json()
            items = data.get("data", [])
            topics = []
            for item in items[:50]:  # å–å‰50ä¸ª
                target = item.get("target", {})
                title = target.get("title", "")
                if title:
                    topics.append(title)
            logger.info(f"  âœ… çŸ¥ä¹: è·å– {len(topics)} ä¸ªçƒ­æ¦œè¯é¢˜")
            return topics
        except Exception as e:
            logger.warning(f"  âš ï¸  çŸ¥ä¹çƒ­æ¦œè§£æå¤±è´¥: {e}")

        return []


class WeiboFetcher(EnhancedFetcher):
    """å¾®åšçƒ­æœæŠ“å–å™¨ï¼ˆéš¾åº¦è¾ƒé«˜ï¼‰"""

    def fetch_hot_topics(self) -> List[str]:
        """æŠ“å–å¾®åšçƒ­æœï¼ˆéœ€è¦ç™»å½•æ€ï¼ŒæˆåŠŸç‡è¾ƒä½ï¼‰"""
        url = "https://weibo.com/ajax/side/hotSearch"

        logger.info(f"  ğŸ” å¾®åšçƒ­æœ")
        response = self.fetch(url, source_name="å¾®åšçƒ­æœ")

        if not response:
            return []

        try:
            data = response.json()
            items = data.get("data", {}).get("realtime", [])
            topics = [item.get("word", "") for item in items]
            logger.info(f"  âœ… å¾®åš: è·å– {len(topics)} ä¸ªçƒ­æœè¯é¢˜")
            return topics
        except Exception as e:
            logger.warning(f"  âš ï¸  å¾®åšçƒ­æœè§£æå¤±è´¥: {e}")

        return []


class GoogleAutoCompleteFetcher(EnhancedFetcher):
    """Googleè‡ªåŠ¨è¡¥å…¨æŠ“å–å™¨ï¼ˆæ–°å¢ï¼‰"""

    def fetch_suggestions(self, keyword: str) -> List[str]:
        """æŠ“å–Googleæœç´¢å»ºè®®"""
        url = "http://suggestqueries.google.com/complete/search"
        params = {
            "client": "youtube",
            "ds": "yt",
            "q": keyword,
            "output": "json"
        }

        logger.info(f"  ğŸ” Google: {keyword}")
        response = self.fetch(url, params=params, source_name="Googleå»ºè®®")

        if not response:
            return []

        try:
            text = response.text
            # Googleè¿”å›çš„æ˜¯JavaScriptä»£ç ï¼Œéœ€è¦è§£æ
            match = re.search(r'\((.*)\)', text)
            if match:
                data = json.loads(match.group(1))
                suggestions = data[1] if len(data) > 1 else []
                logger.info(f"  âœ… Google: è·å– {len(suggestions)} ä¸ªå»ºè®®è¯")
                return suggestions
        except Exception as e:
            logger.warning(f"  âš ï¸  Googleè§£æå¤±è´¥: {e}")

        return []


class BingAutoCompleteFetcher(EnhancedFetcher):
    """Bingè‡ªåŠ¨è¡¥å…¨æŠ“å–å™¨ï¼ˆæ–°å¢ï¼‰"""

    def fetch_suggestions(self, keyword: str) -> List[str]:
        """æŠ“å–Bingæœç´¢å»ºè®®"""
        url = "http://api.bing.com/qsonhs.aspx"
        params = {
            "type": "cb",
            "q": keyword
        }

        logger.info(f"  ğŸ” Bing: {keyword}")
        response = self.fetch(url, params=params, source_name="Bingå»ºè®®")

        if not response:
            return []

        try:
            text = response.text
            match = re.search(r'AS\.AddSugg\((.*)\)', text)
            if match:
                data = json.loads(match.group(1))
                if isinstance(data, dict) and "AS" in data:
                    results = data["AS"]["Results"]
                    suggestions = []
                    for result in results:
                        for suggestion in result.get("Suggs", []):
                            suggestions.append(suggestion.get("Txt", ""))
                    logger.info(f"  âœ… Bing: è·å– {len(suggestions)} ä¸ªå»ºè®®è¯")
                    return suggestions
        except Exception as e:
            logger.warning(f"  âš ï¸  Bingè§£æå¤±è´¥: {e}")

        return []


# å·¥å‚å‡½æ•°
def create_fetcher(source: str, **kwargs) -> EnhancedFetcher:
    """
    åˆ›å»ºæŠ“å–å™¨å®ä¾‹

    Args:
        source: æ•°æ®æºåç§° (baidu/bilibili/taobao/zhihu/weibo/google/bing)
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        å¯¹åº”çš„æŠ“å–å™¨å®ä¾‹
    """
    fetchers = {
        "baidu": BaiduFetcher,
        "bilibili": BilibiliFetcher,
        "taobao": TaobaoFetcher,
        "zhihu": ZhihuFetcher,
        "weibo": WeiboFetcher,
        "google": GoogleAutoCompleteFetcher,
        "bing": BingAutoCompleteFetcher
    }

    fetcher_class = fetchers.get(source.lower(), EnhancedFetcher)
    return fetcher_class(**kwargs)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("=" * 60)
    print("ğŸŒ å¢å¼ºç‰ˆç½‘ç»œæŠ“å–å™¨æµ‹è¯•")
    print("=" * 60)

    test_keyword = "å…»ç”Ÿ"

    # æµ‹è¯•ç™¾åº¦
    print(f"\næµ‹è¯•å…³é”®è¯: {test_keyword}")
    print("-" * 60)

    baidu_fetcher = create_fetcher("baidu")
    baidu_results = baidu_fetcher.fetch_suggestions(test_keyword)
    print(f"ç™¾åº¦ç»“æœ: {baidu_results[:5] if baidu_results else 'æ— '}")

    # æµ‹è¯•Bç«™
    bili_fetcher = create_fetcher("bilibili")
    bili_results = bili_fetcher.fetch_suggestions(test_keyword)
    print(f"Bç«™ç»“æœ: {bili_results[:5] if bili_results else 'æ— '}")

    # æµ‹è¯•Google
    google_fetcher = create_fetcher("google")
    google_results = google_fetcher.fetch_suggestions(test_keyword)
    print(f"Googleç»“æœ: {google_results[:5] if google_results else 'æ— '}")

    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)
